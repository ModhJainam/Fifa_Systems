{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Option 1\n",
    "\n",
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/12 22:23:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "appName = \"Big Data Analytics\"\n",
    "master = \"local[*]\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    "    .set('spark.driver.host','127.0.0.1')\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties={}\n",
    "db_properties['username']=\"postgres\"\n",
    "db_properties['password']=\"\"\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "db_properties['table']=\"fifa.fifa\"\n",
    "db_properties['driver']=\"org.postgresql.Driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_male = spark.read.csv('./Data/players_15.csv', header = True)\n",
    "combined_df = df_male.withColumn(\"year\", lit(2015))\n",
    "combined_df = combined_df.withColumn(\"record_id\", monotonically_increasing_id()).select(\"record_id\", *combined_df.columns)\n",
    "combined_df = combined_df.withColumn(\"gender\", lit(\"Male\"))\n",
    "folder_path = \"./Data\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name == \"players_15.csv\":\n",
    "        continue\n",
    "    year = \"20\" + file_name[-6:-4]\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df_read = spark.read.csv(file_path, header = True)\n",
    "    df_read = df_read.withColumn(\"year\", lit(int(year)))\n",
    "    df_read = df_read.withColumn(\"record_id\", monotonically_increasing_id()).select(\"record_id\", *df_read.columns)\n",
    "    if \"female\" in file_name:\n",
    "        df_read = df_read.withColumn(\"gender\", lit(\"Female\"))\n",
    "    else:\n",
    "        df_read = df_read.withColumn(\"gender\", lit(\"Male\"))\n",
    "    combined_df = combined_df.union(df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/12 22:23:12 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write to PostgreSQL\n",
    "combined_df.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from PostgreSQL to verify\n",
    "df_from_postgres = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['record_id',\n",
       " 'sofifa_id',\n",
       " 'player_url',\n",
       " 'short_name',\n",
       " 'long_name',\n",
       " 'player_positions',\n",
       " 'overall',\n",
       " 'potential',\n",
       " 'value_eur',\n",
       " 'wage_eur',\n",
       " 'age',\n",
       " 'dob',\n",
       " 'height_cm',\n",
       " 'weight_kg',\n",
       " 'club_team_id',\n",
       " 'club_name',\n",
       " 'league_name',\n",
       " 'league_level',\n",
       " 'club_position',\n",
       " 'club_jersey_number',\n",
       " 'club_loaned_from',\n",
       " 'club_joined',\n",
       " 'club_contract_valid_until',\n",
       " 'nationality_id',\n",
       " 'nationality_name',\n",
       " 'nation_team_id',\n",
       " 'nation_position',\n",
       " 'nation_jersey_number',\n",
       " 'preferred_foot',\n",
       " 'weak_foot',\n",
       " 'skill_moves',\n",
       " 'international_reputation',\n",
       " 'work_rate',\n",
       " 'body_type',\n",
       " 'real_face',\n",
       " 'release_clause_eur',\n",
       " 'player_tags',\n",
       " 'player_traits',\n",
       " 'pace',\n",
       " 'shooting',\n",
       " 'passing',\n",
       " 'dribbling',\n",
       " 'defending',\n",
       " 'physic',\n",
       " 'attacking_crossing',\n",
       " 'attacking_finishing',\n",
       " 'attacking_heading_accuracy',\n",
       " 'attacking_short_passing',\n",
       " 'attacking_volleys',\n",
       " 'skill_dribbling',\n",
       " 'skill_curve',\n",
       " 'skill_fk_accuracy',\n",
       " 'skill_long_passing',\n",
       " 'skill_ball_control',\n",
       " 'movement_acceleration',\n",
       " 'movement_sprint_speed',\n",
       " 'movement_agility',\n",
       " 'movement_reactions',\n",
       " 'movement_balance',\n",
       " 'power_shot_power',\n",
       " 'power_jumping',\n",
       " 'power_stamina',\n",
       " 'power_strength',\n",
       " 'power_long_shots',\n",
       " 'mentality_aggression',\n",
       " 'mentality_interceptions',\n",
       " 'mentality_positioning',\n",
       " 'mentality_vision',\n",
       " 'mentality_penalties',\n",
       " 'mentality_composure',\n",
       " 'defending_marking_awareness',\n",
       " 'defending_standing_tackle',\n",
       " 'defending_sliding_tackle',\n",
       " 'goalkeeping_diving',\n",
       " 'goalkeeping_handling',\n",
       " 'goalkeeping_kicking',\n",
       " 'goalkeeping_positioning',\n",
       " 'goalkeeping_reflexes',\n",
       " 'goalkeeping_speed',\n",
       " 'ls',\n",
       " 'st',\n",
       " 'rs',\n",
       " 'lw',\n",
       " 'lf',\n",
       " 'cf',\n",
       " 'rf',\n",
       " 'rw',\n",
       " 'lam',\n",
       " 'cam',\n",
       " 'ram',\n",
       " 'lm',\n",
       " 'lcm',\n",
       " 'cm',\n",
       " 'rcm',\n",
       " 'rm',\n",
       " 'lwb',\n",
       " 'ldm',\n",
       " 'cdm',\n",
       " 'rdm',\n",
       " 'rwb',\n",
       " 'lb',\n",
       " 'lcb',\n",
       " 'cb',\n",
       " 'rcb',\n",
       " 'rb',\n",
       " 'gk',\n",
       " 'player_face_url',\n",
       " 'club_logo_url',\n",
       " 'club_flag_url',\n",
       " 'nation_logo_url',\n",
       " 'nation_flag_url',\n",
       " 'year',\n",
       " 'gender']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_postgres.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144323"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_postgres.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_spark(spark, db_properties):\n",
    "    df_from_postgres = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", db_properties['url'])\\\n",
    "        .option(\"dbtable\", db_properties['table'])\\\n",
    "        .option(\"user\", db_properties['username'])\\\n",
    "        .option(\"password\", db_properties['password'])\\\n",
    "        .option(\"Driver\", db_properties['driver'])\\\n",
    "        .load()\n",
    "    df = df_from_postgres.filter(df_from_postgres[\"gender\"] == \"Male\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_clubs_with_contracts_ending(spark, db_properties, X, Y, Z):\n",
    "    df = read_from_spark(spark, db_properties)\n",
    "    df_filtered = df.filter(col(\"year\") == X)\n",
    "    df_expiring = df_filtered.filter(col(\"club_contract_valid_until\").cast(\"int\") >= Z)\n",
    "    result = df_expiring.groupBy(\"club_name\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .limit(Y)\n",
    "    return result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clubs_by_average_age(spark, db_properties, X, Y, highest=True):\n",
    "    if X <= 0:\n",
    "        return \"X must be a positive integer\"\n",
    "    if Y < 2015 or Y > 2022:\n",
    "        return \"Y must be a year between 2015 and 2022 inclusively\"\n",
    "    df = read_from_spark(spark, db_properties)\n",
    "    \n",
    "    # Filter data for specified year Y\n",
    "    df_filtered = df.filter(col(\"year\") == Y)\n",
    "    avg_age_per_club = df_filtered.groupBy(\"club_name\") \\\n",
    "        .agg(round(avg(\"age\").cast(\"float\"),2).alias(\"average_age\"))\n",
    "    if highest:\n",
    "        sorted_clubs = avg_age_per_club.orderBy(desc(\"average_age\"))\n",
    "    else:\n",
    "        sorted_clubs = avg_age_per_club.orderBy(asc(\"average_age\"))\n",
    "\n",
    "    top_clubs = sorted_clubs.limit(X)\n",
    "    last_club = top_clubs.collect()[-1]\n",
    "    threshold_age = last_club[\"average_age\"]\n",
    "    if highest:\n",
    "        result_clubs = sorted_clubs.filter(col(\"average_age\") >= threshold_age).collect()\n",
    "    else:\n",
    "        result_clubs = sorted_clubs.filter(col(\"average_age\") <= threshold_age).collect()\n",
    "    return result_clubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_popular_nationality(spark, db_properties):\n",
    "    df = read_from_spark(spark, db_properties)\n",
    "    # df_filtered = df.filter((col(\"year\") >= 2015) & (col(\"year\") <= 2022))\n",
    "    nationality_counts = df.groupBy(\"year\", \"nationality_name\") \\\n",
    "        .agg(count(\"*\").alias(\"count\"))\n",
    "    # Create a window partitioned by year and ordered by count descending\n",
    "    window = Window.partitionBy(\"year\").orderBy(desc(\"count\"))\n",
    "    \n",
    "    # Add row number within each year partition\n",
    "    ranked_nationalities = nationality_counts.withColumn(\"rank\", row_number().over(window))\n",
    "    # Filter for the top nationality for each year\n",
    "    most_popular_nationalities = ranked_nationalities.filter(col(\"rank\") == 1) \\\n",
    "        .select(\"year\", \"nationality_name\", \"count\") \\\n",
    "        .orderBy(\"year\")\n",
    "    \n",
    "    return most_popular_nationalities.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task-2.1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clubs = get_top_clubs_with_contracts_ending(spark=spark, db_properties=db_properties, X=2021, Y=10, Z=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(club_name='GwangJu FC', count=28),\n",
       " Row(club_name='Zamora Fútbol Club', count=27),\n",
       " Row(club_name='Club Plaza de Deportes Colonia', count=27),\n",
       " Row(club_name='SL Benfica', count=26),\n",
       " Row(club_name='Club Deportivo El Nacional', count=26),\n",
       " Row(club_name='Sociedad Deportiva Aucas', count=26),\n",
       " Row(club_name='Gangwon FC', count=26),\n",
       " Row(club_name='Club Atlético Nacional Potosí', count=26),\n",
       " Row(club_name='Busan IPark', count=26),\n",
       " Row(club_name='Club Sportivo Luqueño', count=25)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_clubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task-2.2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clubs_by_age = find_clubs_by_average_age(spark=spark, db_properties=db_properties, X=10, Y=2017, highest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(club_name='Sevilla Atlético', average_age=19.920000076293945),\n",
       " Row(club_name='Swindon Town', average_age=21.3700008392334),\n",
       " Row(club_name='CD Huachipato', average_age=21.40999984741211),\n",
       " Row(club_name='FC Nordsjælland', average_age=21.40999984741211),\n",
       " Row(club_name='FC Twente', average_age=21.59000015258789),\n",
       " Row(club_name='Envigado FC', average_age=21.610000610351562),\n",
       " Row(club_name='KRC Genk', average_age=21.6299991607666),\n",
       " Row(club_name='Crewe Alexandra', average_age=21.81999969482422),\n",
       " Row(club_name='Barnsley', average_age=21.8700008392334),\n",
       " Row(club_name='Ajax', average_age=21.969999313354492)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clubs_by_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task-2.3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_nationalities = get_most_popular_nationality(spark=spark, db_properties=db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=2015, nationality_name='England', count=1627),\n",
       " Row(year=2016, nationality_name='England', count=1519),\n",
       " Row(year=2017, nationality_name='England', count=1627),\n",
       " Row(year=2018, nationality_name='England', count=1633),\n",
       " Row(year=2019, nationality_name='England', count=1625),\n",
       " Row(year=2020, nationality_name='England', count=1670),\n",
       " Row(year=2021, nationality_name='England', count=1685),\n",
       " Row(year=2022, nationality_name='England', count=1719)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_nationalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task-3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = read_from_spark(spark, db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define UDF for safe evaluation\n",
    "def safe_eval(x):\n",
    "    try:\n",
    "        return float(eval(str(x)))\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define UDF for trait assignment\n",
    "def assign_traits(player_positions, *traits):\n",
    "    traits = [float(t) if t is not None else None for t in traits]\n",
    "    if player_positions and 'GK' in player_positions:\n",
    "        return traits[6:12]\n",
    "    else:\n",
    "        return traits[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_engineering_pipeline(spark_df):\n",
    "    # Drop columns\n",
    "    columns_to_drop = ['record_id', 'sofifa_id', 'player_url', 'short_name', 'long_name',\n",
    "                       'dob', 'club_name', 'league_name', 'club_position', 'club_jersey_number', 'club_loaned_from',\n",
    "                       'club_contract_valid_until', 'nationality_id', 'nationality_name', 'nation_team_id',\n",
    "                       'nation_position', 'nation_jersey_number', 'real_face', 'player_tags', 'player_traits', \n",
    "                       'player_face_url', 'club_logo_url', 'club_flag_url', 'nation_logo_url', 'nation_flag_url',\n",
    "                       'year', 'gender', 'club_joined']\n",
    "\n",
    "    positions = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw',\n",
    "             'lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm',\n",
    "             'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb',\n",
    "             'rcb', 'rb', 'gk']\n",
    "    attacking_positions = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw',\n",
    "                           'lam', 'cam', 'ram']\n",
    "    midfield_positions = ['lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm',\n",
    "                          'ldm', 'cdm', 'rdm']\n",
    "    defensive_positions = ['rwb', 'lb', 'lcb', 'cb', 'lwb', 'rcb', 'rb',\n",
    "                           'ldm', 'cdm', 'rdm']\n",
    "    gk_positions = ['gk']\n",
    "\n",
    "    # Define main traits\n",
    "    main_traits_to_merge = ['pace', 'shooting', 'passing', 'dribbling', 'defending', 'physic', \n",
    "                            'goalkeeping_diving', 'goalkeeping_handling', 'goalkeeping_kicking',\n",
    "                            'goalkeeping_positioning', 'goalkeeping_reflexes', 'goalkeeping_speed']\n",
    "\n",
    "    # List of skill columns to average\n",
    "    skill_columns = ['skill_dribbling', 'skill_curve', 'skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control']\n",
    "\n",
    "    #Trait1 = pace/goalkeeping_diving\n",
    "    #Trait2 = shooting/goalkeeping_handling\n",
    "    #Trait3 = passing/goalkeeping_kicking\n",
    "    #Trait4 = dribbling/goalkeeping_positioning\n",
    "    #Trait5 = defending/goalkeeping_reflexes\n",
    "    #Trait6 = physic/goalkeeping_speed\n",
    "    \n",
    "    safe_eval_udf = F.udf(safe_eval, FloatType())\n",
    "    assign_traits_udf = udf(assign_traits, ArrayType(FloatType()))\n",
    "    \n",
    "    #Drop Columns\n",
    "    spark_df = spark_df.drop(*columns_to_drop)\n",
    "    \n",
    "    # Apply safe_eval to position columns\n",
    "    for pos in positions:\n",
    "        spark_df = spark_df.withColumn(pos, safe_eval_udf(pos))\n",
    "\n",
    "    #Add new columns 'average_val_attacking', 'average_val_midfield', 'average_val_defensive', 'average_val_gk'\n",
    "    spark_df = spark_df.withColumn(\n",
    "        'average_val_attacking', \n",
    "        F.aggregate(\n",
    "            F.array(*[F.col(pos) for pos in attacking_positions]),\n",
    "            F.lit(0.0),\n",
    "            lambda acc, x: acc + x,\n",
    "            lambda acc: acc / F.size(F.array(*[F.col(pos) for pos in attacking_positions]))\n",
    "        )\n",
    "    )\n",
    "    spark_df = spark_df.withColumn(\n",
    "        'average_val_midfield', \n",
    "        F.aggregate(\n",
    "            F.array(*[F.col(pos) for pos in midfield_positions]),\n",
    "            F.lit(0.0),\n",
    "            lambda acc, x: acc + x,\n",
    "            lambda acc: acc / F.size(F.array(*[F.col(pos) for pos in midfield_positions]))\n",
    "        )\n",
    "    )\n",
    "    spark_df = spark_df.withColumn(\n",
    "        'average_val_defensive', \n",
    "        F.aggregate(\n",
    "            F.array(*[F.col(pos) for pos in defensive_positions]),\n",
    "            F.lit(0.0),\n",
    "            lambda acc, x: acc + x,\n",
    "            lambda acc: acc / F.size(F.array(*[F.col(pos) for pos in defensive_positions]))\n",
    "        )\n",
    "    )\n",
    "    spark_df = spark_df.withColumn(\n",
    "        'average_val_gk', \n",
    "        F.aggregate(\n",
    "            F.array(*[F.col(pos) for pos in gk_positions]),\n",
    "            F.lit(0.0),\n",
    "            lambda acc, x: acc + x,\n",
    "            lambda acc: acc / F.size(F.array(*[F.col(pos) for pos in gk_positions]))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Apply trait assignment\n",
    "    spark_df = spark_df.withColumn('traits', \n",
    "        assign_traits_udf(col('player_positions'), *[col(trait) for trait in main_traits_to_merge])\n",
    "    )\n",
    "    \n",
    "    # Extract individual traits\n",
    "    for i in range(1, 7):\n",
    "        spark_df = spark_df.withColumn(f'trait{i}', col('traits').getItem(i-1))\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    spark_df = spark_df.drop(*positions, *main_traits_to_merge, 'player_positions', 'traits')\n",
    "    \n",
    "    # Drop rows with null values in specific columns\n",
    "    spark_df = spark_df.na.drop(subset=['wage_eur', 'value_eur', 'trait6'])\n",
    "    \n",
    "    # Drop additional columns\n",
    "    spark_df = spark_df.drop('release_clause_eur', 'league_level')\n",
    "    \n",
    "    # Fill null values in club_team_id with -1\n",
    "    spark_df = spark_df.fillna({'club_team_id': -1})\n",
    "\n",
    "    #Average out skills as highly correlated\n",
    "    spark_df = spark_df.withColumn(\n",
    "        'corr_av_skills', \n",
    "        F.aggregate(\n",
    "            F.array(*[F.col(pos) for pos in skill_columns]),\n",
    "            F.lit(0.0),\n",
    "            lambda acc, x: acc + x,\n",
    "            lambda acc: acc / F.size(F.array(*[F.col(pos) for pos in skill_columns]))\n",
    "        )\n",
    "    )\n",
    "    #Drop the skills as added one new average skill column\n",
    "    spark_df = spark_df.drop('skill_dribbling', 'skill_curve', 'skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control')\n",
    "\n",
    "    #Drop movement_acceleration as highly correlated to movement_speed\n",
    "    spark_df = spark_df.drop('movement_acceleration')\n",
    "\n",
    "    #Drop movement_acceleration as highly correlated to movement_speed\n",
    "    spark_df = spark_df.drop('preferred_foot')\n",
    "\n",
    "    s = spark_df.toPandas()\n",
    "    print(s)\n",
    "    print(f'Shape {s.shape}')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       overall potential value_eur wage_eur age height_cm weight_kg  \\\n",
      "0           56        71  350000.0    700.0  19       178        64   \n",
      "1           56        66  325000.0   2000.0  20       180        70   \n",
      "2           56        67  300000.0   2000.0  22       183        77   \n",
      "3           56        65  300000.0    500.0  19       186        77   \n",
      "4           56        66  325000.0   3000.0  21       180        71   \n",
      "...        ...       ...       ...      ...  ..       ...       ...   \n",
      "140176      59        62  160000.0   3000.0  25       183        76   \n",
      "140177      59        67  230000.0   2000.0  22       177        75   \n",
      "140178      59        73  350000.0   2000.0  20       176        70   \n",
      "140179      59        67  240000.0   3000.0  23       169        57   \n",
      "140180      59        63  160000.0   3000.0  25       176        74   \n",
      "\n",
      "       club_team_id weak_foot skill_moves  ... average_val_midfield  \\\n",
      "0             226.0         3           2  ...            57.090909   \n",
      "1           15009.0         5           2  ...            46.272727   \n",
      "2          101151.0         2           2  ...            46.909091   \n",
      "3          112552.0         2           2  ...            48.909091   \n",
      "4             209.0         5           2  ...            46.545455   \n",
      "...             ...       ...         ...  ...                  ...   \n",
      "140176     112510.0         3           1  ...            23.727273   \n",
      "140177       1443.0         3           2  ...            53.000000   \n",
      "140178         17.0         3           2  ...            52.454545   \n",
      "140179     112395.0         4           3  ...            50.909091   \n",
      "140180     112172.0         3           2  ...            52.636364   \n",
      "\n",
      "       average_val_defensive average_val_gk trait1 trait2 trait3 trait4  \\\n",
      "0                       51.0           15.0   68.0   46.0   56.0   60.0   \n",
      "1                       36.4           16.0   59.0   55.0   38.0   55.0   \n",
      "2                       56.4           15.0   61.0   29.0   46.0   39.0   \n",
      "3                       56.5           18.0   49.0   30.0   45.0   46.0   \n",
      "4                       37.7           17.0   68.0   57.0   41.0   52.0   \n",
      "...                      ...            ...    ...    ...    ...    ...   \n",
      "140176                  22.6           59.0   61.0   60.0   54.0   59.0   \n",
      "140177                  44.2           16.0   77.0   51.0   54.0   58.0   \n",
      "140178                  39.4           14.0   61.0   57.0   58.0   65.0   \n",
      "140179                  38.5           16.0   91.0   55.0   45.0   66.0   \n",
      "140180                  57.5           16.0   70.0   40.0   51.0   58.0   \n",
      "\n",
      "       trait5 trait6 corr_av_skills  \n",
      "0        42.0   47.0           54.8  \n",
      "1        19.0   59.0           40.4  \n",
      "2        60.0   51.0           39.6  \n",
      "3        60.0   44.0           38.0  \n",
      "4        22.0   56.0           40.2  \n",
      "...       ...    ...            ...  \n",
      "140176   59.0   31.0           16.4  \n",
      "140177   27.0   61.0           51.4  \n",
      "140178   24.0   50.0           59.4  \n",
      "140179   19.0   48.0           43.6  \n",
      "140180   58.0   59.0           49.8  \n",
      "\n",
      "[140181 rows x 47 columns]\n",
      "Shape (140181, 47)\n"
     ]
    }
   ],
   "source": [
    "s = data_engineering_pipeline(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From correlation analysis\n",
    "average - skill columns\n",
    "remove - movement_acceleration\n",
    "Preffered foot is not correlated to overall. so drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body_type\n",
       "Normal (170-185)    49318\n",
       "Lean (170-185)      30690\n",
       "Normal (185+)       28963\n",
       "Lean (185+)         14365\n",
       "Normal (170-)        4891\n",
       "Stocky (170-185)     4646\n",
       "Lean (170-)          3330\n",
       "Stocky (185+)        2848\n",
       "Stocky (170-)         822\n",
       "Unique                308\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['body_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "work_rate\n",
       "Medium/Medium    77614\n",
       "High/Medium      23843\n",
       "Medium/High      12441\n",
       "Medium/Low        6897\n",
       "High/High         6660\n",
       "High/Low          5585\n",
       "Low/Medium        3513\n",
       "Low/High          3339\n",
       "Low/Low            289\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['work_rate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = s\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('overall', axis=1)\n",
    "y = df['overall']\n",
    "\n",
    "# Split the data into train+validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split train+validation into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have X_train, X_val, X_test\n",
    "\n",
    "# Define the order for work_rate\n",
    "work_rate_order = ['Low/Low', 'Low/Medium', 'Low/High',\n",
    "                   'Medium/Low', 'Medium/Medium', 'Medium/High',\n",
    "                   'High/Low', 'High/Medium', 'High/High']\n",
    "\n",
    "# Initialize OrdinalEncoder for work_rate\n",
    "work_rate_encoder = OrdinalEncoder(categories=[work_rate_order])\n",
    "\n",
    "# Encode work_rate\n",
    "X_train['work_rate_encoded'] = work_rate_encoder.fit_transform(X_train[['work_rate']])\n",
    "X_val['work_rate_encoded'] = work_rate_encoder.transform(X_val[['work_rate']])\n",
    "X_test['work_rate_encoded'] = work_rate_encoder.transform(X_test[['work_rate']])\n",
    "\n",
    "# Perform one-hot encoding for body_type using pd.get_dummies\n",
    "X_train_body_type_dummies = pd.get_dummies(X_train['body_type'], prefix='body_type')\n",
    "X_val_body_type_dummies = pd.get_dummies(X_val['body_type'], prefix='body_type')\n",
    "X_test_body_type_dummies = pd.get_dummies(X_test['body_type'], prefix='body_type')\n",
    "\n",
    "# Combine the original DataFrames with the encoded columns\n",
    "X_train = pd.concat([X_train, X_train_body_type_dummies], axis=1)\n",
    "X_val = pd.concat([X_val, X_val_body_type_dummies], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_body_type_dummies], axis=1)\n",
    "\n",
    "# Drop the original 'work_rate' and 'body_type' columns\n",
    "X_train = X_train.drop(['work_rate', 'body_type'], axis=1)\n",
    "X_val = X_val.drop(['work_rate', 'body_type'], axis=1)\n",
    "X_test = X_test.drop(['work_rate', 'body_type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_train = pd.to_numeric(y_train)\n",
    "y_val = pd.to_numeric(y_val)\n",
    "y_test = pd.to_numeric(y_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values).reshape(-1, 1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).reshape(-1, 1).to(device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "potential  value_eur  wage_eur  age    height_cm  weight_kg  club_team_id  weak_foot  skill_moves  international_reputation  attacking_crossing  attacking_finishing  attacking_heading_accuracy  attacking_short_passing  attacking_volleys  movement_sprint_speed  movement_agility  movement_reactions  movement_balance  power_shot_power  power_jumping  power_stamina  power_strength  power_long_shots  mentality_aggression  mentality_interceptions  mentality_positioning  mentality_vision  mentality_penalties  mentality_composure  defending_marking_awareness  defending_standing_tackle  defending_sliding_tackle  average_val_attacking  average_val_midfield  average_val_defensive  average_val_gk  trait1  trait2  trait3  trait4  trait5  trait6  corr_av_skills  work_rate_encoded  body_type_Lean (170-)  body_type_Lean (170-185)  body_type_Lean (185+)  body_type_Normal (170-)  body_type_Normal (170-185)  body_type_Normal (185+)  body_type_Stocky (170-)  body_type_Stocky (170-185)  body_type_Stocky (185+)  body_type_Unique\n",
       "False      False      False     False  False      False      False         False      False        False                     False               False                False                       False                    False              False                  False             False               False             False             False          False          False           False             False                 False                    False                  False             False                False                False                        False                      False                     False                  False                 False                  False           False   False   False   False   False   False   False           False              False                  False                     False                  False                    False                       False                    False                    False                       False                    False               69858\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            True                 False                        False                      False                     False                  False                 False                  False           False   False   False   False   False   False   False           False              False                  False                     False                  False                    False                       False                    False                    False                       False                    False               19857\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nan = X_train.columns[X_train.isna().any()].tolist()\n",
    "\n",
    "# Check for columns with None values (which are also considered NaN in pandas)\n",
    "columns_with_none = X_train.columns[X_train.isnull().any()].tolist()\n",
    "\n",
    "# Combine the results (they should be the same in most cases)\n",
    "columns_with_missing = list(set(columns_with_nan + columns_with_none))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mentality_composure']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mentality_composure\n",
       "False    108947\n",
       "True      31234\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['mentality_composure'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           59\n",
       "1           49\n",
       "2           46\n",
       "3           52\n",
       "4           47\n",
       "          ... \n",
       "140176    None\n",
       "140177    None\n",
       "140178    None\n",
       "140179    None\n",
       "140180    None\n",
       "Name: mentality_composure, Length: 140181, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['mentality_composure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'fit_intercept': True}\n",
      "Best cross-validated MSE: 3.2425979362708715\n",
      "Validation MSE: 3.269571653684482\n",
      "Validation R^2: 0.9350467685918915\n",
      "Test MSE: 3.2342667447441444\n",
      "Test R^2: 0.9343777293350956\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming X_train, X_val, X_test, y_train, y_val, y_test are already defined\n",
    "\n",
    "# Create the Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "# linear_model.fit(X_train.drop(['mentality_composure'], axis=1), y_train)\n",
    "# # Define a simple parameter grid for hyperparameter tuning\n",
    "param_grid = {'fit_intercept': [True, False]}\n",
    "\n",
    "# Set up the grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=linear_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Fit the grid search model on the training data\n",
    "grid_search.fit(X_train.drop(['mentality_composure'], axis=1), y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_  # Convert negative MSE to positive\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best cross-validated MSE: {best_score}\")\n",
    "\n",
    "# Train the Linear Regression model with the best parameters on the full training set\n",
    "best_linear_model = LinearRegression(**best_params)\n",
    "best_linear_model.fit(X_train.drop(['mentality_composure'], axis=1), y_train)\n",
    "\n",
    "# Validate on validation data\n",
    "val_predictions = best_linear_model.predict(X_val.drop(['mentality_composure'], axis=1))\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "val_r2 = r2_score(y_val, val_predictions)\n",
    "\n",
    "print(f\"Validation MSE: {val_mse}\")\n",
    "print(f\"Validation R^2: {val_r2}\")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_predictions = best_linear_model.predict(X_test.drop(['mentality_composure'], axis=1))\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test R^2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  24.9s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=  57.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aravjain/miniforge3/envs/spark_env/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  28.8s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=  56.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  30.8s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=  56.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  24.7s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time= 1.0min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  24.9s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  28.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=  42.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  14.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  25.7s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  28.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=  42.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  13.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  27.5s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  29.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=  42.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  18.8s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=  56.8s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  37.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  24.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  25.7s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  34.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=  41.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.9min\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  38.1s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  43.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  17.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  16.7s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  28.8s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  25.3s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  29.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=  56.1s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  43.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  16.9s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  28.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  39.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time= 1.0min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  39.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time= 1.0min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  47.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  59.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  28.3s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  40.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time= 1.0min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  50.5s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  43.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  16.7s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  37.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.5min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  47.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  42.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  16.9s\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  27.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  40.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time= 1.0min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=  40.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time= 1.0min\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.1min\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.1min\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.1min\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.1min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  28.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  56.7s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time= 1.5min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  27.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time= 1.1min\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time= 1.5min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  28.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  56.5s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time= 1.5min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  28.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  57.5s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time= 1.5min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  34.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  56.8s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time= 1.5min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  38.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=  44.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  52.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  47.9s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.9min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time= 1.4min\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.9min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time= 1.4min\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.9min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time= 1.5min\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 2.0min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  39.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=  44.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  47.8s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.9min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  27.7s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  36.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  28.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  35.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=  44.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  53.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  40.2s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  28.0s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  37.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  28.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time= 1.4min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time= 1.1min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  27.8s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  37.5s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  28.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  35.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=  44.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  53.7s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  40.0s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  27.8s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  37.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  28.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time= 1.4min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  44.8s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  31.6s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  20.5s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time= 1.2min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  44.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  20.1s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time= 1.2min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  33.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  19.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  19.7s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time= 1.2min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  43.1s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  23.8s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time= 1.2min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  57.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  35.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time=  44.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=  52.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  39.8s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  28.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  37.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  28.4s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time= 1.2min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  13.6s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  13.4s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  35.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  20.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  39.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  20.2s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  35.5s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  35.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  26.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  39.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=  51.7s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  56.7s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  22.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  15.1s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.9min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  21.5s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  39.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=  52.3s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  56.5s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  22.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  15.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.9min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  57.9s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  21.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  13.2s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.9min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  17.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  39.7s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  41.9s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  36.7s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  40.9s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  34.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  42.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  36.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  41.2s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  33.9s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  21.1s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  39.7s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=  52.7s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  57.1s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  22.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  15.1s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  41.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  33.6s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 2.2min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 1.5min\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  13.5s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  30.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  11.4s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  21.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 1.5min\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  14.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  31.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  12.1s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  21.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  42.4s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  36.9s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.9min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 1.5min\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  14.2s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  31.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  11.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  21.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  13.5s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  35.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time=  52.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  56.9s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  22.0s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  14.8s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  41.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  33.9s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 1.6min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  21.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  36.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  44.1s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.6min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.6min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  41.8s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  37.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.9min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 1.5min\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  14.1s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=400; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  31.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  10.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time= 1.2min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  18.8s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  36.5s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  44.4s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  34.6s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  19.6s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  23.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time= 1.5min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 1.5min\n",
      "[CV] END max_depth=50, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  13.9s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  31.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  10.9s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  18.9s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  37.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  44.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  35.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  19.5s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  23.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time= 1.5min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  13.1s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  35.4s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  41.9s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  36.8s\n",
      "[CV] END max_depth=40, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  41.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  33.8s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time= 1.2min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 1.6min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  20.8s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  49.4s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  42.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  34.7s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  19.1s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  24.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time= 1.4min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.5min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  35.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  19.6s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  23.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aravjain/miniforge3/envs/spark_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "150 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "150 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aravjain/miniforge3/envs/spark_env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/aravjain/miniforge3/envs/spark_env/lib/python3.9/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/aravjain/miniforge3/envs/spark_env/lib/python3.9/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/aravjain/miniforge3/envs/spark_env/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/aravjain/miniforge3/envs/spark_env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.9776341  0.98705584 0.97769849        nan 0.98733396 0.98732917\n",
      " 0.98062404 0.98893135 0.98931225        nan 0.98912725 0.97753862\n",
      "        nan 0.9872709  0.97770191        nan        nan 0.98822784\n",
      " 0.9879073  0.98734024        nan 0.98920269 0.98855095 0.9884273\n",
      "        nan        nan 0.98941278 0.98061113 0.98062787 0.98871136\n",
      " 0.98763879        nan 0.9872709  0.98954432        nan        nan\n",
      " 0.98849928 0.98722381 0.98927004 0.98058764 0.98894207 0.98821033\n",
      "        nan        nan 0.98848507 0.98060947 0.98712066        nan\n",
      "        nan        nan 0.98899206 0.98840074 0.98931897 0.98733329\n",
      "        nan        nan        nan 0.98848472 0.98866928        nan\n",
      " 0.9888125  0.98894207 0.98709919        nan 0.98912725        nan\n",
      "        nan 0.98954432 0.98824609        nan 0.98742939 0.98860771\n",
      " 0.9894304  0.98705584        nan        nan 0.98903022        nan\n",
      " 0.98886811 0.98782001 0.98057065        nan 0.98903872 0.9888827\n",
      " 0.98866339 0.98859664 0.98814235 0.98838467 0.98861995 0.98822784\n",
      "        nan 0.9873858         nan 0.98851602 0.98806113 0.98845455\n",
      " 0.98922959 0.98952603 0.98891621 0.98056219]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 50}\n",
      "Validation MSE: 0.5154926366757322\n",
      "Validation R^2: 0.9897592357453177\n",
      "Test MSE: 0.49938213689053756\n",
      "Test R^2: 0.9898676910908777\n",
      "Top 10 Most Important Features:\n",
      "                  feature  importance\n",
      "1               value_eur    0.178956\n",
      "17     movement_reactions    0.109087\n",
      "2                wage_eur    0.107092\n",
      "33   average_val_midfield    0.068376\n",
      "39                 trait4    0.057930\n",
      "32  average_val_attacking    0.053525\n",
      "40                 trait5    0.049282\n",
      "34  average_val_defensive    0.037945\n",
      "0               potential    0.037661\n",
      "38                 trait3    0.032995\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 1.6min\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time= 1.3min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  19.6s\n",
      "[CV] END max_depth=50, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  36.4s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  43.9s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=50, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  34.7s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  19.4s\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  24.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=400; total time= 1.5min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time= 1.5min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time= 1.5min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time= 1.2min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  19.4s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.5min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time= 1.2min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  28.7s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.5min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time= 1.2min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  28.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 1.6min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time= 1.0min\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=400; total time= 1.2min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  19.2s\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time= 1.4min\n",
      "[CV] END max_depth=40, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.5min\n",
      "[CV] END max_depth=40, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time= 1.2min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  28.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  27.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  27.2s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming X_train, X_val, X_test, y_train, y_val, y_test are already defined\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(estimator=rf_model, param_distributions=param_grid, \n",
    "                               n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train.drop(['mentality_composure'], axis=1), y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = rf_random.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Create a new model with the best parameters\n",
    "best_rf_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "\n",
    "# Train the model on the full training set\n",
    "best_rf_model.fit(X_train.drop(['mentality_composure'], axis=1), y_train)\n",
    "\n",
    "# Validate the model\n",
    "val_predictions = best_rf_model.predict(X_val.drop(['mentality_composure'], axis=1))\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "val_r2 = r2_score(y_val, val_predictions)\n",
    "\n",
    "print(f\"Validation MSE: {val_mse}\")\n",
    "print(f\"Validation R^2: {val_r2}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = best_rf_model.predict(X_test.drop(['mentality_composure'], axis=1))\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test R^2: {test_r2}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = best_rf_model.feature_importances_\n",
    "feature_names = X_train.drop(['mentality_composure'], axis=1).columns\n",
    "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Multi-layer Perceptron (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, in_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features, in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x += residual\n",
    "        return x\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_size, num_blocks):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.fc_in = nn.Linear(input_size, 64)\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(num_blocks)])\n",
    "        self.fc_out = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "def tune_hyperparameters(model_class, param_grid):\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        current_params = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {current_params}\")\n",
    "\n",
    "        if model_class == MLP:\n",
    "            model = model_class(input_size=X_train.shape[1], hidden_size=current_params['hidden_size'])\n",
    "        else:  # ResNet\n",
    "            model = model_class(input_size=X_train.shape[1], num_blocks=current_params['num_blocks'])\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=current_params['lr'])\n",
    "        criterion = nn.MSELoss()\n",
    "        train_loader = DataLoader(train_dataset, batch_size=current_params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=current_params['batch_size'])\n",
    "\n",
    "        val_loss = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = current_params\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "    return best_params, best_val_loss, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids\n",
    "mlp_param_grid = {\n",
    "    'lr': [0.001, 0.01],\n",
    "    'batch_size': [32, 64],\n",
    "    'hidden_size': [32, 64]\n",
    "}\n",
    "\n",
    "resnet_param_grid = {\n",
    "    'lr': [0.001, 0.01],\n",
    "    'batch_size': [32, 64],\n",
    "    'num_blocks': [2, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning MLP model...\n",
      "Training with parameters: {'lr': 0.001, 'batch_size': 32, 'hidden_size': 32}\n",
      "Epoch [10/100], Train Loss: 1626.1440, Val Loss: 1515.4728\n",
      "Epoch [20/100], Train Loss: 214.3058, Val Loss: 181.3124\n",
      "Epoch [30/100], Train Loss: 50.2396, Val Loss: 50.3376\n",
      "Epoch [40/100], Train Loss: 50.2309, Val Loss: 50.3357\n",
      "Epoch [50/100], Train Loss: 50.2324, Val Loss: 50.3351\n",
      "Epoch [60/100], Train Loss: 50.2326, Val Loss: 50.3372\n",
      "Epoch [70/100], Train Loss: 50.2295, Val Loss: 50.3342\n",
      "Epoch [80/100], Train Loss: 50.2333, Val Loss: 50.3374\n",
      "Epoch [90/100], Train Loss: 50.2329, Val Loss: 50.3384\n",
      "Epoch [100/100], Train Loss: 50.2337, Val Loss: 50.3380\n",
      "Training with parameters: {'lr': 0.001, 'batch_size': 32, 'hidden_size': 64}\n",
      "Epoch [10/100], Train Loss: 1611.7924, Val Loss: 1501.6126\n",
      "Epoch [20/100], Train Loss: 209.9265, Val Loss: 177.4705\n",
      "Epoch [30/100], Train Loss: 50.2353, Val Loss: 50.3365\n",
      "Epoch [40/100], Train Loss: 50.2306, Val Loss: 50.3363\n",
      "Epoch [50/100], Train Loss: 50.2304, Val Loss: 50.3359\n",
      "Epoch [60/100], Train Loss: 50.2314, Val Loss: 50.3361\n",
      "Epoch [70/100], Train Loss: 50.2344, Val Loss: 50.3375\n",
      "Epoch [80/100], Train Loss: 50.2305, Val Loss: 50.3359\n",
      "Epoch [90/100], Train Loss: 50.2321, Val Loss: 50.3372\n",
      "Epoch [100/100], Train Loss: 50.2371, Val Loss: 50.3368\n",
      "Training with parameters: {'lr': 0.001, 'batch_size': 64, 'hidden_size': 32}\n",
      "Epoch [10/100], Train Loss: 2823.0398, Val Loss: 2746.2657\n",
      "Epoch [20/100], Train Loss: 1566.2825, Val Loss: 1509.9769\n",
      "Epoch [30/100], Train Loss: 692.4282, Val Loss: 656.3769\n",
      "Epoch [40/100], Train Loss: 195.5866, Val Loss: 179.3016\n",
      "Epoch [50/100], Train Loss: 50.8223, Val Loss: 50.4719\n",
      "Epoch [60/100], Train Loss: 50.2341, Val Loss: 50.3022\n",
      "Epoch [70/100], Train Loss: 50.2358, Val Loss: 50.3031\n",
      "Epoch [80/100], Train Loss: 50.2344, Val Loss: 50.3005\n",
      "Epoch [90/100], Train Loss: 50.2376, Val Loss: 50.3024\n",
      "Epoch [100/100], Train Loss: 50.2367, Val Loss: 50.3020\n",
      "Training with parameters: {'lr': 0.001, 'batch_size': 64, 'hidden_size': 64}\n",
      "Epoch [10/100], Train Loss: 2814.3064, Val Loss: 2737.6716\n",
      "Epoch [20/100], Train Loss: 1559.8491, Val Loss: 1503.6675\n",
      "Epoch [30/100], Train Loss: 688.3027, Val Loss: 652.3765\n",
      "Epoch [40/100], Train Loss: 193.7065, Val Loss: 177.5261\n",
      "Epoch [50/100], Train Loss: 50.7672, Val Loss: 50.4490\n",
      "Epoch [60/100], Train Loss: 50.2321, Val Loss: 50.3017\n",
      "Epoch [70/100], Train Loss: 50.2329, Val Loss: 50.3018\n",
      "Epoch [80/100], Train Loss: 50.2364, Val Loss: 50.3003\n",
      "Epoch [90/100], Train Loss: 50.2333, Val Loss: 50.3025\n",
      "Epoch [100/100], Train Loss: 50.2316, Val Loss: 50.3047\n",
      "Training with parameters: {'lr': 0.01, 'batch_size': 32, 'hidden_size': 32}\n",
      "Epoch [10/100], Train Loss: 50.2430, Val Loss: 50.3358\n",
      "Epoch [20/100], Train Loss: 50.2406, Val Loss: 50.3342\n",
      "Epoch [30/100], Train Loss: 50.2385, Val Loss: 50.3375\n",
      "Epoch [40/100], Train Loss: 50.2374, Val Loss: 50.3380\n",
      "Epoch [50/100], Train Loss: 50.2452, Val Loss: 50.3388\n",
      "Epoch [60/100], Train Loss: 50.2441, Val Loss: 50.3381\n",
      "Epoch [70/100], Train Loss: 50.2387, Val Loss: 50.3408\n",
      "Epoch [80/100], Train Loss: 50.2398, Val Loss: 50.3358\n",
      "Epoch [90/100], Train Loss: 50.2403, Val Loss: 50.3341\n",
      "Epoch [100/100], Train Loss: 50.2421, Val Loss: 50.3568\n",
      "Training with parameters: {'lr': 0.01, 'batch_size': 32, 'hidden_size': 64}\n",
      "Epoch [10/100], Train Loss: 50.2429, Val Loss: 50.3353\n",
      "Epoch [20/100], Train Loss: 50.2436, Val Loss: 50.3415\n",
      "Epoch [30/100], Train Loss: 50.2443, Val Loss: 50.3343\n",
      "Epoch [40/100], Train Loss: 50.2418, Val Loss: 50.3355\n",
      "Epoch [50/100], Train Loss: 50.2399, Val Loss: 50.3382\n",
      "Epoch [60/100], Train Loss: 50.2371, Val Loss: 50.3495\n",
      "Epoch [70/100], Train Loss: 50.2446, Val Loss: 50.3382\n",
      "Epoch [80/100], Train Loss: 50.2399, Val Loss: 50.3465\n",
      "Epoch [90/100], Train Loss: 50.2392, Val Loss: 50.3370\n",
      "Epoch [100/100], Train Loss: 50.2400, Val Loss: 50.3529\n",
      "Training with parameters: {'lr': 0.01, 'batch_size': 64, 'hidden_size': 32}\n",
      "Epoch [10/100], Train Loss: 50.2373, Val Loss: 50.3031\n",
      "Epoch [20/100], Train Loss: 50.2383, Val Loss: 50.3074\n",
      "Epoch [30/100], Train Loss: 50.2380, Val Loss: 50.3066\n",
      "Epoch [40/100], Train Loss: 50.2375, Val Loss: 50.3046\n",
      "Epoch [50/100], Train Loss: 50.2353, Val Loss: 50.3091\n",
      "Epoch [60/100], Train Loss: 50.2392, Val Loss: 50.3172\n",
      "Epoch [70/100], Train Loss: 50.2332, Val Loss: 50.3004\n",
      "Epoch [80/100], Train Loss: 50.2380, Val Loss: 50.3126\n",
      "Epoch [90/100], Train Loss: 50.2387, Val Loss: 50.3139\n",
      "Epoch [100/100], Train Loss: 50.2387, Val Loss: 50.3030\n",
      "Training with parameters: {'lr': 0.01, 'batch_size': 64, 'hidden_size': 64}\n",
      "Epoch [10/100], Train Loss: 50.2413, Val Loss: 50.2991\n",
      "Epoch [20/100], Train Loss: 50.2360, Val Loss: 50.3210\n",
      "Epoch [30/100], Train Loss: 50.2366, Val Loss: 50.3099\n",
      "Epoch [40/100], Train Loss: 50.2377, Val Loss: 50.3095\n",
      "Epoch [50/100], Train Loss: 50.2363, Val Loss: 50.2993\n",
      "Epoch [60/100], Train Loss: 50.2421, Val Loss: 50.2991\n",
      "Epoch [70/100], Train Loss: 50.2348, Val Loss: 50.3001\n",
      "Epoch [80/100], Train Loss: 50.2385, Val Loss: 50.2991\n",
      "Epoch [90/100], Train Loss: 50.2389, Val Loss: 50.3009\n",
      "Epoch [100/100], Train Loss: 50.2392, Val Loss: 50.3087\n",
      "Best MLP parameters: {'lr': 0.001, 'batch_size': 64, 'hidden_size': 32}\n",
      "Best MLP validation loss: 50.3019530467498\n"
     ]
    }
   ],
   "source": [
    "# Tune and train models\n",
    "print(\"Tuning MLP model...\")\n",
    "mlp_best_params, mlp_best_loss, mlp_best_model = tune_hyperparameters(MLP, mlp_param_grid)\n",
    "print(f\"Best MLP parameters: {mlp_best_params}\")\n",
    "print(f\"Best MLP validation loss: {mlp_best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTuning ResNet model...\")\n",
    "resnet_best_params, resnet_best_loss, resnet_best_model = tune_hyperparameters(ResNet, resnet_param_grid)\n",
    "print(f\"Best ResNet parameters: {resnet_best_params}\")\n",
    "print(f\"Best ResNet validation loss: {resnet_best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, in_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features, in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x += residual\n",
    "        return x\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_size, num_blocks):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.fc_in = nn.Linear(input_size, 64)\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(num_blocks)])\n",
    "        self.fc_out = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "# Hyperparameter tuning\n",
    "def tune_hyperparameters(model_class, param_grid):\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        current_params = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {current_params}\")\n",
    "\n",
    "        if model_class == MLP:\n",
    "            model = model_class(input_size=X_train.shape[1], hidden_size=current_params['hidden_size'])\n",
    "        else:  # ResNet\n",
    "            model = model_class(input_size=X_train.shape[1], num_blocks=current_params['num_blocks'])\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=current_params['lr'])\n",
    "        criterion = nn.MSELoss()\n",
    "        train_loader = DataLoader(train_dataset, batch_size=current_params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=current_params['batch_size'])\n",
    "\n",
    "        val_loss = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = current_params\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "    return best_params, best_val_loss, best_model\n",
    "\n",
    "# Define hyperparameter grids\n",
    "mlp_param_grid = {\n",
    "    'lr': [0.001, 0.01],\n",
    "    'batch_size': [32, 64],\n",
    "    'hidden_size': [32, 64]\n",
    "}\n",
    "\n",
    "resnet_param_grid = {\n",
    "    'lr': [0.001, 0.01],\n",
    "    'batch_size': [32, 64],\n",
    "    'num_blocks': [2, 3]\n",
    "}\n",
    "\n",
    "# Tune and train models\n",
    "print(\"Tuning MLP model...\")\n",
    "mlp_best_params, mlp_best_loss, mlp_best_model = tune_hyperparameters(MLP, mlp_param_grid)\n",
    "print(f\"Best MLP parameters: {mlp_best_params}\")\n",
    "print(f\"Best MLP validation loss: {mlp_best_loss}\")\n",
    "\n",
    "print(\"\\nTuning ResNet model...\")\n",
    "resnet_best_params, resnet_best_loss, resnet_best_model = tune_hyperparameters(ResNet, resnet_param_grid)\n",
    "print(f\"Best ResNet parameters: {resnet_best_params}\")\n",
    "print(f\"Best ResNet validation loss: {resnet_best_loss}\")\n",
    "\n",
    "# Save best models\n",
    "torch.save(mlp_best_model, 'best_mlp_model.pth')\n",
    "torch.save(resnet_best_model, 'best_resnet_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

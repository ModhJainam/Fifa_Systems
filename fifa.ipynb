{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Option 1\n",
    "\n",
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "import warnings\n",
    "import torch\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from itertools import product\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "appName = \"Big Data Analytics\"\n",
    "master = \"local[*]\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    " .set('spark.driver.host','127.0.0.1')\\\n",
    "   .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties={}\n",
    "db_properties['username']=\"postgres\"\n",
    "db_properties['password']=\"systems\"\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "db_properties['table']=\"fifa.fifa\"\n",
    "db_properties['driver']=\"org.postgresql.Driver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_male = spark.read.csv('./Data/players_15.csv', header = True)\n",
    "combined_df = df_male.withColumn(\"year\", lit(2015))\n",
    "# combined_df = combined_df.withColumn(\"record_id\", monotonically_increasing_id()).select(\"record_id\", *combined_df.columns)\n",
    "combined_df = combined_df.withColumn(\"gender\", lit(\"Male\"))\n",
    "folder_path = \"./Data\"\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name == \"players_15.csv\":\n",
    "        continue\n",
    "    year = \"20\" + file_name[-6:-4]\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df_read = spark.read.csv(file_path, header = True)\n",
    "    df_read = df_read.withColumn(\"year\", lit(int(year)))\n",
    "    # df_read = df_read.withColumn(\"record_id\", monotonically_increasing_id()).select(\"record_id\", *df_read.columns)\n",
    "    if \"female\" in file_name:\n",
    "        df_read = df_read.withColumn(\"gender\", lit(\"Female\"))\n",
    "    else:\n",
    "        df_read = df_read.withColumn(\"gender\", lit(\"Male\"))\n",
    "    combined_df = combined_df.union(df_read)\n",
    "combined_df = combined_df.withColumn(\"record_id\", monotonically_increasing_id()).select(\"record_id\", *combined_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write to PostgreSQL\n",
    "combined_df.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from PostgreSQL to verify\n",
    "df_from_postgres = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['record_id',\n",
       " 'sofifa_id',\n",
       " 'player_url',\n",
       " 'short_name',\n",
       " 'long_name',\n",
       " 'player_positions',\n",
       " 'overall',\n",
       " 'potential',\n",
       " 'value_eur',\n",
       " 'wage_eur',\n",
       " 'age',\n",
       " 'dob',\n",
       " 'height_cm',\n",
       " 'weight_kg',\n",
       " 'club_team_id',\n",
       " 'club_name',\n",
       " 'league_name',\n",
       " 'league_level',\n",
       " 'club_position',\n",
       " 'club_jersey_number',\n",
       " 'club_loaned_from',\n",
       " 'club_joined',\n",
       " 'club_contract_valid_until',\n",
       " 'nationality_id',\n",
       " 'nationality_name',\n",
       " 'nation_team_id',\n",
       " 'nation_position',\n",
       " 'nation_jersey_number',\n",
       " 'preferred_foot',\n",
       " 'weak_foot',\n",
       " 'skill_moves',\n",
       " 'international_reputation',\n",
       " 'work_rate',\n",
       " 'body_type',\n",
       " 'real_face',\n",
       " 'release_clause_eur',\n",
       " 'player_tags',\n",
       " 'player_traits',\n",
       " 'pace',\n",
       " 'shooting',\n",
       " 'passing',\n",
       " 'dribbling',\n",
       " 'defending',\n",
       " 'physic',\n",
       " 'attacking_crossing',\n",
       " 'attacking_finishing',\n",
       " 'attacking_heading_accuracy',\n",
       " 'attacking_short_passing',\n",
       " 'attacking_volleys',\n",
       " 'skill_dribbling',\n",
       " 'skill_curve',\n",
       " 'skill_fk_accuracy',\n",
       " 'skill_long_passing',\n",
       " 'skill_ball_control',\n",
       " 'movement_acceleration',\n",
       " 'movement_sprint_speed',\n",
       " 'movement_agility',\n",
       " 'movement_reactions',\n",
       " 'movement_balance',\n",
       " 'power_shot_power',\n",
       " 'power_jumping',\n",
       " 'power_stamina',\n",
       " 'power_strength',\n",
       " 'power_long_shots',\n",
       " 'mentality_aggression',\n",
       " 'mentality_interceptions',\n",
       " 'mentality_positioning',\n",
       " 'mentality_vision',\n",
       " 'mentality_penalties',\n",
       " 'mentality_composure',\n",
       " 'defending_marking_awareness',\n",
       " 'defending_standing_tackle',\n",
       " 'defending_sliding_tackle',\n",
       " 'goalkeeping_diving',\n",
       " 'goalkeeping_handling',\n",
       " 'goalkeeping_kicking',\n",
       " 'goalkeeping_positioning',\n",
       " 'goalkeeping_reflexes',\n",
       " 'goalkeeping_speed',\n",
       " 'ls',\n",
       " 'st',\n",
       " 'rs',\n",
       " 'lw',\n",
       " 'lf',\n",
       " 'cf',\n",
       " 'rf',\n",
       " 'rw',\n",
       " 'lam',\n",
       " 'cam',\n",
       " 'ram',\n",
       " 'lm',\n",
       " 'lcm',\n",
       " 'cm',\n",
       " 'rcm',\n",
       " 'rm',\n",
       " 'lwb',\n",
       " 'ldm',\n",
       " 'cdm',\n",
       " 'rdm',\n",
       " 'rwb',\n",
       " 'lb',\n",
       " 'lcb',\n",
       " 'cb',\n",
       " 'rcb',\n",
       " 'rb',\n",
       " 'gk',\n",
       " 'player_face_url',\n",
       " 'club_logo_url',\n",
       " 'club_flag_url',\n",
       " 'nation_logo_url',\n",
       " 'nation_flag_url',\n",
       " 'year',\n",
       " 'gender']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_postgres.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144323"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_postgres.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_spark(spark, db_properties):\n",
    "    df_from_postgres = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", db_properties['url'])\\\n",
    "        .option(\"dbtable\", db_properties['table'])\\\n",
    "        .option(\"user\", db_properties['username'])\\\n",
    "        .option(\"password\", db_properties['password'])\\\n",
    "        .option(\"Driver\", db_properties['driver'])\\\n",
    "        .load()\n",
    "    df = df_from_postgres.filter(df_from_postgres[\"gender\"] == \"Male\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_clubs_with_contracts_ending(spark, db_properties, X, Y, Z):\n",
    "    df = read_from_spark(spark, db_properties)\n",
    "    df_filtered = df.filter(col(\"year\") == X)\n",
    "    df_expiring = df_filtered.filter(col(\"club_contract_valid_until\").cast(\"int\") >= Z)\n",
    "    result = df_expiring.groupBy(\"club_name\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .limit(Y)\n",
    "    return result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clubs_by_average_age(spark, db_properties, X, Y, highest=True):\n",
    "    if X <= 0:\n",
    "        return \"X must be a positive integer\"\n",
    "    if Y < 2015 or Y > 2022:\n",
    "        return \"Y must be a year between 2015 and 2022 inclusively\"\n",
    "    df = read_from_spark(spark, db_properties)\n",
    "    \n",
    "    # Filter data for specified year Y\n",
    "    df_filtered = df.filter(col(\"year\") == Y)\n",
    "    avg_age_per_club = df_filtered.groupBy(\"club_name\") \\\n",
    "        .agg(round(avg(\"age\").cast(\"float\"),2).alias(\"average_age\"))\n",
    "    if highest:\n",
    "        sorted_clubs = avg_age_per_club.orderBy(desc(\"average_age\"))\n",
    "    else:\n",
    "        sorted_clubs = avg_age_per_club.orderBy(asc(\"average_age\"))\n",
    "\n",
    "    top_clubs = sorted_clubs.limit(X)\n",
    "    last_club = top_clubs.collect()[-1]\n",
    "    threshold_age = last_club[\"average_age\"]\n",
    "    if highest:\n",
    "        result_clubs = sorted_clubs.filter(col(\"average_age\") >= threshold_age).collect()\n",
    "    else:\n",
    "        result_clubs = sorted_clubs.filter(col(\"average_age\") <= threshold_age).collect()\n",
    "    return result_clubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_popular_nationality(spark, db_properties):\n",
    "    df = read_from_spark(spark, db_properties)\n",
    "    # df_filtered = df.filter((col(\"year\") >= 2015) & (col(\"year\") <= 2022))\n",
    "    nationality_counts = df.groupBy(\"year\", \"nationality_name\") \\\n",
    "        .agg(count(\"*\").alias(\"count\"))\n",
    "    # Create a window partitioned by year and ordered by count descending\n",
    "    window = Window.partitionBy(\"year\").orderBy(desc(\"count\"))\n",
    "    \n",
    "    # Add row number within each year partition\n",
    "    ranked_nationalities = nationality_counts.withColumn(\"rank\", row_number().over(window))\n",
    "    # Filter for the top nationality for each year\n",
    "    most_popular_nationalities = ranked_nationalities.filter(col(\"rank\") == 1) \\\n",
    "        .select(\"year\", \"nationality_name\", \"count\") \\\n",
    "        .orderBy(\"year\")\n",
    "    \n",
    "    return most_popular_nationalities.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clubs = get_top_clubs_with_contracts_ending(spark=spark, db_properties=db_properties, X=2021, Y=10, Z=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(club_name='GwangJu FC', count=28),\n",
       " Row(club_name='Zamora Fútbol Club', count=27),\n",
       " Row(club_name='Club Plaza de Deportes Colonia', count=27),\n",
       " Row(club_name='SL Benfica', count=26),\n",
       " Row(club_name='Club Deportivo El Nacional', count=26),\n",
       " Row(club_name='Sociedad Deportiva Aucas', count=26),\n",
       " Row(club_name='Gangwon FC', count=26),\n",
       " Row(club_name='Club Atlético Nacional Potosí', count=26),\n",
       " Row(club_name='Busan IPark', count=26),\n",
       " Row(club_name='Club Sportivo Luqueño', count=25)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_clubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task-2.2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "clubs_by_age = find_clubs_by_average_age(spark=spark, db_properties=db_properties, X=10, Y=2017, highest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(club_name='Sevilla Atlético', average_age=19.920000076293945),\n",
       " Row(club_name='Swindon Town', average_age=21.3700008392334),\n",
       " Row(club_name='CD Huachipato', average_age=21.40999984741211),\n",
       " Row(club_name='FC Nordsjælland', average_age=21.40999984741211),\n",
       " Row(club_name='FC Twente', average_age=21.59000015258789),\n",
       " Row(club_name='Envigado FC', average_age=21.610000610351562),\n",
       " Row(club_name='KRC Genk', average_age=21.6299991607666),\n",
       " Row(club_name='Crewe Alexandra', average_age=21.81999969482422),\n",
       " Row(club_name='Barnsley', average_age=21.8700008392334),\n",
       " Row(club_name='Ajax', average_age=21.969999313354492)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clubs_by_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task-2.3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_nationalities = get_most_popular_nationality(spark=spark, db_properties=db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year=2015, nationality_name='England', count=1627),\n",
       " Row(year=2016, nationality_name='England', count=1519),\n",
       " Row(year=2017, nationality_name='England', count=1627),\n",
       " Row(year=2018, nationality_name='England', count=1633),\n",
       " Row(year=2019, nationality_name='England', count=1625),\n",
       " Row(year=2020, nationality_name='England', count=1670),\n",
       " Row(year=2021, nationality_name='England', count=1685),\n",
       " Row(year=2022, nationality_name='England', count=1719)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_nationalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Refer to preprocessing.ipynb for detailed investigation on correlations and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup column categories\n",
    "col_names = ['record_id',\n",
    " 'sofifa_id',\n",
    " 'player_url',\n",
    " 'short_name',\n",
    " 'long_name',\n",
    " 'player_positions',\n",
    " 'overall',\n",
    " 'potential',\n",
    " 'value_eur',\n",
    " 'wage_eur',\n",
    " 'age',\n",
    " 'dob',\n",
    " 'height_cm',\n",
    " 'weight_kg',\n",
    " 'club_team_id',\n",
    " 'club_name',\n",
    " 'league_name',\n",
    " 'league_level',\n",
    " 'club_position',\n",
    " 'club_jersey_number',\n",
    " 'club_loaned_from',\n",
    " 'club_joined',\n",
    " 'club_contract_valid_until',\n",
    " 'nationality_id',\n",
    " 'nationality_name',\n",
    " 'nation_team_id',\n",
    " 'nation_position',\n",
    " 'nation_jersey_number',\n",
    " 'preferred_foot',\n",
    " 'weak_foot',\n",
    " 'skill_moves',\n",
    " 'international_reputation',\n",
    " 'work_rate',\n",
    " 'body_type',\n",
    " 'real_face',\n",
    " 'release_clause_eur',\n",
    " 'player_tags',\n",
    " 'player_traits',\n",
    " 'pace',\n",
    " 'shooting',\n",
    " 'passing',\n",
    " 'dribbling',\n",
    " 'defending',\n",
    " 'physic',\n",
    " 'attacking_crossing',\n",
    " 'attacking_finishing',\n",
    " 'attacking_heading_accuracy',\n",
    " 'attacking_short_passing',\n",
    " 'attacking_volleys',\n",
    " 'skill_dribbling',\n",
    " 'skill_curve',\n",
    " 'skill_fk_accuracy',\n",
    " 'skill_long_passing',\n",
    " 'skill_ball_control',\n",
    " 'movement_acceleration',\n",
    " 'movement_sprint_speed',\n",
    " 'movement_agility',\n",
    " 'movement_reactions',\n",
    " 'movement_balance',\n",
    " 'power_shot_power',\n",
    " 'power_jumping',\n",
    " 'power_stamina',\n",
    " 'power_strength',\n",
    " 'power_long_shots',\n",
    " 'mentality_aggression',\n",
    " 'mentality_interceptions',\n",
    " 'mentality_positioning',\n",
    " 'mentality_vision',\n",
    " 'mentality_penalties',\n",
    " 'mentality_composure',\n",
    " 'defending_marking_awareness',\n",
    " 'defending_standing_tackle',\n",
    " 'defending_sliding_tackle',\n",
    " 'goalkeeping_diving',\n",
    " 'goalkeeping_handling',\n",
    " 'goalkeeping_kicking',\n",
    " 'goalkeeping_positioning',\n",
    " 'goalkeeping_reflexes',\n",
    " 'goalkeeping_speed',\n",
    " 'ls',\n",
    " 'st',\n",
    " 'rs',\n",
    " 'lw',\n",
    " 'lf',\n",
    " 'cf',\n",
    " 'rf',\n",
    " 'rw',\n",
    " 'lam',\n",
    " 'cam',\n",
    " 'ram',\n",
    " 'lm',\n",
    " 'lcm',\n",
    " 'cm',\n",
    " 'rcm',\n",
    " 'rm',\n",
    " 'lwb',\n",
    " 'ldm',\n",
    " 'cdm',\n",
    " 'rdm',\n",
    " 'rwb',\n",
    " 'lb',\n",
    " 'lcb',\n",
    " 'cb',\n",
    " 'rcb',\n",
    " 'rb',\n",
    " 'gk',\n",
    " 'player_face_url',\n",
    " 'club_logo_url',\n",
    " 'club_flag_url',\n",
    " 'nation_logo_url',\n",
    " 'nation_flag_url',\n",
    " 'year',\n",
    " 'gender']\n",
    "\n",
    "ordinal_cols = ['work_rate']\n",
    "nominal_cols = ['body_type']\n",
    "continuous_cols = ['potential', 'value_eur', 'wage_eur', 'age', 'height_cm', 'weight_kg',\n",
    "                   'weak_foot', 'skill_moves', 'international_reputation',\n",
    "                   'attacking_crossing','attacking_finishing', 'attacking_heading_accuracy', 'attacking_short_passing', 'attacking_volleys',\n",
    "                   'movement_sprint_speed', 'movement_agility', 'movement_reactions', 'movement_balance',\n",
    "                   'power_shot_power', 'power_jumping', 'power_stamina', 'power_strength', 'power_long_shots',\n",
    "                   'mentality_aggression', 'mentality_interceptions', 'mentality_positioning', 'mentality_vision', 'mentality_penalties',\n",
    "                   'defending_marking_awareness', 'defending_standing_tackle', 'defending_sliding_tackle']\n",
    "\n",
    "main_traits_cols = ['pace', 'shooting', 'passing', 'dribbling', 'defending', 'physic', \n",
    "                            'goalkeeping_diving', 'goalkeeping_handling', 'goalkeeping_kicking',\n",
    "                            'goalkeeping_positioning', 'goalkeeping_reflexes', 'goalkeeping_speed']\n",
    "\n",
    "skill_cols = ['skill_dribbling', 'skill_curve', 'skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control']\n",
    "\n",
    "position_cols = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm',\n",
    "                   'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk']\n",
    "\n",
    "needed_cols = ['record_id', 'player_positions'] #needed for preprocessing but will be dropped\n",
    "\n",
    "columns_to_drop = ['sofifa_id', 'player_url', 'short_name', 'long_name',\n",
    "                   'dob', 'club_name', 'league_name', 'club_position', 'club_jersey_number', 'club_loaned_from',\n",
    "                   'club_joined','club_contract_valid_until', 'nationality_id', 'nationality_name', 'nation_team_id',\n",
    "                   'nation_position', 'nation_jersey_number', 'preferred_foot', 'real_face', 'player_tags', 'player_traits',\n",
    "                   'player_face_url', 'club_logo_url', 'club_flag_url', 'nation_logo_url', 'nation_flag_url',\n",
    "                   'year', 'gender', 'league_level']\n",
    "\n",
    "null_cols_drop = ['release_clause_eur']\n",
    "\n",
    "corr_cols_drop = ['movement_acceleration']\n",
    "\n",
    "dropped_null_rows = ['value_eur', 'wage_eur', 'trait6']\n",
    "\n",
    "imputed_cols = ['mentality_composure', 'club_team_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NullImputer(Transformer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset.withColumn(\"club_team_id_imputed\", when(col(\"club_team_id\").isNull(), -1).otherwise(col(\"club_team_id\")))\n",
    "        return output_df\n",
    "    \n",
    "class MLImputer(Transformer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        s = dataset.select(['record_id','mentality_composure', 'mentality_aggression', 'mentality_interceptions', 'mentality_positioning', 'mentality_vision', 'mentality_penalties']).toPandas()\n",
    "       \n",
    "        # Prepare the data\n",
    "        train_data = s.dropna(subset=['mentality_composure'])\n",
    "        X = train_data[['mentality_aggression', 'mentality_interceptions', 'mentality_positioning', 'mentality_vision', 'mentality_penalties']]\n",
    "        Y = train_data['mentality_composure']\n",
    "\n",
    "        # Fit the linear regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, Y)\n",
    "\n",
    "        # Predict missing values\n",
    "        X_missing = s[s['mentality_composure'].isnull()][['mentality_aggression', 'mentality_interceptions', 'mentality_positioning', 'mentality_vision', 'mentality_penalties']]\n",
    "        predicted_values = model.predict(X_missing)\n",
    "        rounded_predicted_values = np.round(predicted_values).astype(int)\n",
    "        # Impute the predicted values\n",
    "        s.loc[s['mentality_composure'].isnull(), 'mentality_composure'] = rounded_predicted_values\n",
    "        s = s.rename(columns = {'mentality_composure': 'mentality_composure_imputed'})\n",
    "        spark_s = spark.createDataFrame(s[['record_id','mentality_composure_imputed']])\n",
    "        spark_df = dataset.join(spark_s, on = 'record_id', how = 'left')\n",
    "        return spark_df\n",
    "    \n",
    "class AverageSkillCreator(Transformer):\n",
    "    def __init__(self, average_skills_cols = None):\n",
    "        super().__init__()\n",
    "        self.average_skills_cols = average_skills_cols\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        spark_df = dataset\n",
    "        spark_df = spark_df.withColumn(\n",
    "        'corr_av_skills', \n",
    "        aggregate(\n",
    "            array(*[col(pos) for pos in self.average_skills_cols]),\n",
    "            lit(0.0),\n",
    "            lambda acc, x: acc + x,\n",
    "            lambda acc: acc / size(array(*[col(pos) for pos in self.average_skills_cols]))\n",
    "        )\n",
    "        )\n",
    "\n",
    "        return spark_df\n",
    "        \n",
    "class AveragePositionCreator(Transformer):\n",
    "    def __init__(self, position_cols = None):\n",
    "        super().__init__()\n",
    "        self.position_cols = position_cols\n",
    "\n",
    "    # Define UDF for safe evaluation\n",
    "    def safe_eval(self, x):\n",
    "        try:\n",
    "            return float(eval(str(x)))\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def _transform(self, dataset):\n",
    "        attacking_positions = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw',\n",
    "                           'lam', 'cam', 'ram']\n",
    "        midfield_positions = ['lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm',\n",
    "                            'ldm', 'cdm', 'rdm']\n",
    "        defensive_positions = ['rwb', 'lb', 'lcb', 'cb', 'lwb', 'rcb', 'rb',\n",
    "                            'ldm', 'cdm', 'rdm']\n",
    "        gk_positions = ['gk']\n",
    "\n",
    "        spark_df = dataset\n",
    "        safe_eval_udf = udf(self.safe_eval, FloatType())\n",
    "\n",
    "        #Apply safe_eval to each position\n",
    "        for pos in position_cols:\n",
    "            spark_df = spark_df.withColumn(pos, safe_eval_udf(pos))\n",
    "\n",
    "         #Add new columns 'average_val_attacking', 'average_val_midfield', 'average_val_defensive', 'average_val_gk'\n",
    "        spark_df = spark_df.withColumn(\n",
    "            'average_val_attacking', \n",
    "            aggregate(\n",
    "                array(*[col(pos) for pos in attacking_positions]),\n",
    "                lit(0.0),\n",
    "                lambda acc, x: acc + x,\n",
    "                lambda acc: acc / size(array(*[col(pos) for pos in attacking_positions]))\n",
    "            )\n",
    "        )\n",
    "        spark_df = spark_df.withColumn(\n",
    "            'average_val_midfield', \n",
    "            aggregate(\n",
    "                array(*[col(pos) for pos in midfield_positions]),\n",
    "                lit(0.0),\n",
    "                lambda acc, x: acc + x,\n",
    "                lambda acc: acc / size(array(*[col(pos) for pos in midfield_positions]))\n",
    "            )\n",
    "        )\n",
    "        spark_df = spark_df.withColumn(\n",
    "            'average_val_defensive', \n",
    "            aggregate(\n",
    "                array(*[col(pos) for pos in defensive_positions]),\n",
    "                lit(0.0),\n",
    "                lambda acc, x: acc + x,\n",
    "                lambda acc: acc / size(array(*[col(pos) for pos in defensive_positions]))\n",
    "            )\n",
    "        )\n",
    "        spark_df = spark_df.withColumn(\n",
    "            'average_val_gk', \n",
    "            aggregate(\n",
    "                array(*[col(pos) for pos in gk_positions]),\n",
    "                lit(0.0),\n",
    "                lambda acc, x: acc + x,\n",
    "                lambda acc: acc / size(array(*[col(pos) for pos in gk_positions]))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return spark_df\n",
    "\n",
    "def assign_traits(player_positions, *traits):\n",
    "        #Trait1 = pace/goalkeeping_diving\n",
    "        #Trait2 = shooting/goalkeeping_handling\n",
    "        #Trait3 = passing/goalkeeping_kicking\n",
    "        #Trait4 = dribbling/goalkeeping_positioning\n",
    "        #Trait5 = defending/goalkeeping_reflexes\n",
    "        #Trait6 = physic/goalkeeping_speed\n",
    "\n",
    "        traits = [float(t) if t is not None else None for t in traits]\n",
    "        if player_positions and 'GK' in player_positions:\n",
    "            return traits[6:12]\n",
    "        else:\n",
    "            return traits[:6]\n",
    "\n",
    "class MergeMainTraits(Transformer):\n",
    "    def __init__(self, main_traits_cols = None):\n",
    "        super().__init__()\n",
    "        self.main_traits_cols = main_traits_cols\n",
    "        \n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        assign_traits_udf = udf(assign_traits, ArrayType(FloatType()))\n",
    "        output_df = output_df.withColumn('traits', \n",
    "                                       assign_traits_udf(col('player_positions'), *[col(trait) for trait in self.main_traits_cols]))\n",
    "        for i in range(1, 7):\n",
    "            output_df = output_df.withColumn(f'trait{i}', col('traits').getItem(i-1))\n",
    "        \n",
    "        output_df = output_df.drop('traits')\n",
    "        return output_df\n",
    "    \n",
    "class DropOutliers(Transformer):\n",
    "    def __init__(self, continuous_cols = None):\n",
    "        super().__init__()\n",
    "        self.continuous_cols = continuous_cols\n",
    "\n",
    "    def column_add(self, a,b):\n",
    "        return  a.__add__(b)\n",
    "    \n",
    "    def find_outliers(self, df, continuous_cols):\n",
    "        # Identifying the numerical columns in a spark dataframe\n",
    "\n",
    "        # Using the `for` loop to create new columns by identifying the outliers for each feature\n",
    "        for column in continuous_cols:\n",
    "\n",
    "            less_Q1 = 'less_Q1_{}'.format(column)\n",
    "            more_Q3 = 'more_Q3_{}'.format(column)\n",
    "            Q1 = 'Q1_{}'.format(column)\n",
    "            Q3 = 'Q3_{}'.format(column)\n",
    "\n",
    "            # Q1 : First Quartile ., Q3 : Third Quartile\n",
    "            Q1 = df.approxQuantile(column,[0.25],relativeError=0)\n",
    "            Q3 = df.approxQuantile(column,[0.75],relativeError=0)\n",
    "            \n",
    "            # IQR : Inter Quantile Range\n",
    "            # We need to define the index [0], as Q1 & Q3 are a set of lists., to perform a mathematical operation\n",
    "            # Q1 & Q3 are defined seperately so as to have a clear indication on First Quantile & 3rd Quantile\n",
    "            IQR = Q3[0] - Q1[0]\n",
    "            \n",
    "            #selecting the data, with -1.5*IQR to + 1.5*IQR., where param = 1.5 default value\n",
    "            less_Q1 =  Q1[0] - 1.5*IQR\n",
    "            more_Q3 =  Q3[0] + 1.5*IQR\n",
    "            \n",
    "            isOutlierCol = 'is_outlier_{}'.format(column)\n",
    "            \n",
    "            df = df.withColumn(isOutlierCol,when((df[column] > more_Q3) | (df[column] < less_Q1), 1).otherwise(0))\n",
    "        \n",
    "\n",
    "        # Selecting the specific columns which we have added above, to check if there are any outliers\n",
    "        selected_columns = [column for column in df.columns if column.startswith(\"is_outlier\")]\n",
    "        # Adding all the outlier columns into a new colum \"total_outliers\", to see the total number of outliers\n",
    "        df = df.withColumn('total_outliers',reduce(self.column_add, ( df[col] for col in  selected_columns)))\n",
    "\n",
    "        # Dropping the extra columns created above, just to create nice dataframe., without extra columns\n",
    "        df = df.drop(*[column for column in df.columns if column.startswith(\"is_outlier\")])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        df_outliers = self.find_outliers(dataset, continuous_cols)\n",
    "        df_no_outliers = df_outliers.filter(df_outliers['total_outliers']<=6) #Drop all rows with more than 6 outliers\n",
    "        df_no_outliers = df_no_outliers.drop(\"total_outliers\")\n",
    "        return df_no_outliers    \n",
    "     \n",
    "class DropNullRows(Transformer):\n",
    "    def __init__(self, dropped_null_rows = None):\n",
    "        super().__init__()\n",
    "        self.null_cols_drop = dropped_null_rows\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset.na.drop(subset=self.null_cols_drop)\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in (continuous_cols + imputed_cols):\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "        return output_df\n",
    "    \n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing_pipeline():\n",
    "    #Drop all initial columns that are not needed\n",
    "    stage_dropper = ColumnDropper(columns_to_drop = columns_to_drop + null_cols_drop + corr_cols_drop)\n",
    "\n",
    "    # Stage where columns are casted as appropriate types\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "    #Engineer features\n",
    "    stage_maintraits = MergeMainTraits(main_traits_cols)\n",
    "    trait_cols = ['trait1', 'trait2', 'trait3', 'trait4', 'trait5', 'trait6']\n",
    "\n",
    "    stage_average_positions = AveragePositionCreator(position_cols)\n",
    "    avg_position_cols = ['average_val_attacking', 'average_val_midfield', 'average_val_defensive', 'average_val_gk']\n",
    "   \n",
    "    stage_average_skills = AverageSkillCreator(skill_cols)\n",
    "    avg_skill_cols = [\"corr_av_skills\"]\n",
    "\n",
    "    #Impute features\n",
    "    imputed_id_cols = [x+\"_imputed\" for x in imputed_cols]\n",
    "    stage_imputer = NullImputer()\n",
    "    stage_ml_imputer = MLImputer()\n",
    "    \n",
    "    #Stage where null rows are dropped based on certain features\n",
    "    stage_null_dropper = DropNullRows(dropped_null_rows)\n",
    "\n",
    "    #Stage where outlier rows are dropped for continuous features\n",
    "    stage_outlier_dropper = DropOutliers(continuous_cols)\n",
    "\n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n",
    "\n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "    # Stage where ordinal columns are transformed to index columns using StringIndexer\n",
    "    ordinal_id_cols = [x+\"_index\" for x in ordinal_cols]\n",
    "    stage_ordinal_indexer = StringIndexer(inputCols = ordinal_cols, outputCols = ordinal_id_cols )\n",
    "\n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = continuous_cols + nominal_onehot_cols + ordinal_id_cols + trait_cols + avg_position_cols + avg_skill_cols + imputed_id_cols #[imputed_id_cols[1]]\n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "    \n",
    "    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = feature_cols + main_traits_cols + \n",
    "                                         position_cols + skill_cols + imputed_cols + needed_cols + ordinal_cols + nominal_cols + nominal_id_cols +['vectorized_features'])\n",
    "    \n",
    "    # Connect the columns into a pipeline\n",
    "    '''\n",
    "    pipeline = Pipeline(stages=[stage_dropper, stage_typecaster, stage_maintraits, stage_average_positions, stage_average_skills,\n",
    "                                stage_imputer, stage_ml_imputer, stage_null_dropper, stage_outlier_dropper,\n",
    "                                stage_nominal_indexer, stage_nominal_onehot_encoder, stage_ordinal_indexer,\n",
    "                                stage_vector_assembler, stage_scaler, stage_column_dropper])\n",
    "    '''\n",
    "\n",
    "    pipeline = Pipeline(stages=[stage_dropper, stage_typecaster, stage_maintraits, stage_average_positions, stage_average_skills,\n",
    "                                stage_imputer, stage_ml_imputer, stage_null_dropper, stage_nominal_indexer, stage_nominal_onehot_encoder, stage_ordinal_indexer,\n",
    "                                stage_vector_assembler, stage_scaler, stage_column_dropper])\n",
    "    # pipeline = Pipeline(stages=[stage_ml_imputer])\n",
    "\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_from_spark(spark, db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 623:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|overall|            features|\n",
      "+-------+--------------------+\n",
      "|     86|[14.0216019436758...|\n",
      "|     86|[14.4996111008466...|\n",
      "|     85|[13.5435927865050...|\n",
      "|     64|[10.9942106149276...|\n",
      "|     69|[11.1535470006512...|\n",
      "|     69|[10.9942106149276...|\n",
      "|     69|[11.4722197720984...|\n",
      "|     66|[12.5875744721635...|\n",
      "|     58|[11.1535470006512...|\n",
      "|     64|[11.7908925435455...|\n",
      "|     65|[10.8348742292040...|\n",
      "|     69|[11.4722197720984...|\n",
      "|     64|[12.2689017007163...|\n",
      "|     63|[10.6755378434804...|\n",
      "|     80|[12.7469108578871...|\n",
      "|     57|[11.631556157822,...|\n",
      "|     69|[11.4722197720984...|\n",
      "|     80|[12.7469108578871...|\n",
      "|     78|[12.4282380864399...|\n",
      "|     63|[10.0381923005861...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline = get_preprocessing_pipeline()\n",
    "preprocess_model = pipeline.fit(df)\n",
    "df_clean = preprocess_model.transform(df)\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140181"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "s = df_clean.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = s.drop('overall', axis=1)\n",
    "y = s['overall']\n",
    "# Split the data into train+validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Further split train+validation into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20032</th>\n",
       "      <td>[10.03819230058611, 0.05622992354921244, 0.224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79267</th>\n",
       "      <td>[12.109565314992768, 0.02768242390115074, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36530</th>\n",
       "      <td>[10.516201457756877, 0.09515833216020567, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86580</th>\n",
       "      <td>[12.906247243610713, 1.9031666432041132, 2.245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46651</th>\n",
       "      <td>[12.268901700716357, 0.8304727170345222, 1.347...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35697</th>\n",
       "      <td>[12.587574472163535, 0.4671409033319187, 0.314...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72930</th>\n",
       "      <td>[10.994210614927644, 0.1470628769748633, 0.224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139331</th>\n",
       "      <td>[10.197528686309699, 0.022491969419684975, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41597</th>\n",
       "      <td>[12.428238086439945, 1.2111060456753449, 0.898...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73126</th>\n",
       "      <td>[14.021601943675837, 2.0761817925863055, 1.347...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89715 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 features\n",
       "20032   [10.03819230058611, 0.05622992354921244, 0.224...\n",
       "79267   [12.109565314992768, 0.02768242390115074, 0.08...\n",
       "36530   [10.516201457756877, 0.09515833216020567, 0.08...\n",
       "86580   [12.906247243610713, 1.9031666432041132, 2.245...\n",
       "46651   [12.268901700716357, 0.8304727170345222, 1.347...\n",
       "...                                                   ...\n",
       "35697   [12.587574472163535, 0.4671409033319187, 0.314...\n",
       "72930   [10.994210614927644, 0.1470628769748633, 0.224...\n",
       "139331  [10.197528686309699, 0.022491969419684975, 0.0...\n",
       "41597   [12.428238086439945, 1.2111060456753449, 0.898...\n",
       "73126   [14.021601943675837, 2.0761817925863055, 1.347...\n",
       "\n",
       "[89715 rows x 1 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(np.array(X_train['features'].values.tolist(), np.float32))\n",
    "X_val = torch.from_numpy(np.array(X_val['features'].values.tolist(), np.float32))\n",
    "X_test = torch.from_numpy(np.array(X_test['features'].values.tolist(), np.float32))\n",
    "y_train = torch.from_numpy(np.array(y_train, np.float32))\n",
    "y_val = torch.from_numpy(np.array(y_val, np.float32))\n",
    "y_test = torch.from_numpy(np.array(y_test, np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).reshape(-1, 1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Multi-layer Perceptron (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_true = []\n",
    "        train_pred = []\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            train_true.extend(targets.cpu().numpy())\n",
    "            train_pred.extend(outputs.detach().cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_true = []\n",
    "        val_pred = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                \n",
    "                val_true.extend(targets.cpu().numpy())\n",
    "                val_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_r2 = r2_score(train_true, train_pred)\n",
    "        val_r2 = r2_score(val_true, val_pred)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, Train R2: {train_r2:.4f}, '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val R2: {val_r2:.4f}')\n",
    "    \n",
    "    return val_loss, val_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(model_class, param_grid):\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    best_val_r2 = -float('inf')\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        current_params = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Training with parameters: {current_params}\")\n",
    "\n",
    "        if model_class == MLP:\n",
    "            model = model_class(input_size=X_train.shape[1], hidden_size=current_params['hidden_size'])\n",
    "        elif model_class == LinearRegression:\n",
    "            model = model_class(input_size=X_train.shape[1])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model class\")\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=current_params['lr'])\n",
    "        criterion = nn.MSELoss()\n",
    "        train_loader = DataLoader(train_dataset, batch_size=current_params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=current_params['batch_size'])\n",
    "\n",
    "        val_loss, val_r2 = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_r2 = val_r2\n",
    "            best_val_loss = val_loss\n",
    "            best_params = current_params\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "    return best_params, best_val_loss, best_model, best_val_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(model_class):\n",
    "    input_size = X_test_tensor.shape[1]\n",
    "    if model_class == \"LR\":\n",
    "        model = LinearRegression(input_size)\n",
    "        model.load_state_dict(torch.load('./best_linear_model.pth'))\n",
    "    elif model_class == \"MLP\":\n",
    "        model = MLP(input_size, mlp_best_params['hidden_size'])\n",
    "        model.load_state_dict(torch.load('./best_mlp_model.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "    y_pred_np = y_pred.cpu().numpy()\n",
    "    y_test_np = y_test_tensor.cpu().numpy()\n",
    "    \n",
    "    r2 = r2_score(y_test_np, y_pred_np)\n",
    "    mse = mean_squared_error(y_test_np, y_pred_np)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids\n",
    "# mlp_param_grid = {\n",
    "#     'lr': [0.001, 0.01],\n",
    "#     'batch_size': [32, 64],\n",
    "#     'hidden_size': [32, 64]\n",
    "# }\n",
    "# linear_param_grid = {\n",
    "#     'lr': [0.001, 0.01],\n",
    "#     'batch_size': [32, 64]\n",
    "# }\n",
    "\n",
    "mlp_param_grid = {\n",
    "    'lr': [0.01],\n",
    "    'batch_size': [64],\n",
    "    'hidden_size': [64]\n",
    "}\n",
    "\n",
    "linear_param_grid = {\n",
    "    'lr': [0.01],\n",
    "    'batch_size': [64]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning MLP model...\n",
      "Training with parameters: {'lr': 0.01, 'batch_size': 64, 'hidden_size': 64}\n",
      "Epoch [10/70], Train Loss: 1.2457, Train R2: 0.9752, Val Loss: 1.3324, Val R2: 0.9732\n",
      "Epoch [20/70], Train Loss: 0.9164, Train R2: 0.9818, Val Loss: 0.9247, Val R2: 0.9814\n",
      "Epoch [30/70], Train Loss: 0.8698, Train R2: 0.9827, Val Loss: 0.7432, Val R2: 0.9850\n",
      "Epoch [40/70], Train Loss: 0.8380, Train R2: 0.9833, Val Loss: 0.7297, Val R2: 0.9853\n",
      "Epoch [50/70], Train Loss: 0.8181, Train R2: 0.9837, Val Loss: 0.6856, Val R2: 0.9862\n",
      "Epoch [60/70], Train Loss: 0.7801, Train R2: 0.9845, Val Loss: 0.6990, Val R2: 0.9859\n",
      "Epoch [70/70], Train Loss: 0.7582, Train R2: 0.9849, Val Loss: 0.6846, Val R2: 0.9862\n",
      "Best MLP parameters: {'lr': 0.01, 'batch_size': 64, 'hidden_size': 64}\n",
      "Best MLP validation loss: 0.6845993040463864\n",
      "Best MLP validation loss: 0.9862275437987229\n"
     ]
    }
   ],
   "source": [
    "# Tune and train models\n",
    "print(\"Tuning MLP model...\")\n",
    "mlp_best_params, mlp_best_loss, mlp_best_model, mlp_best_r2 = tune_hyperparameters(MLP, mlp_param_grid)\n",
    "print(f\"Best MLP parameters: {mlp_best_params}\")\n",
    "print(f\"Best MLP validation loss: {mlp_best_loss}\")\n",
    "print(f\"Best MLP validation r2: {mlp_best_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Linear Regression model...\n",
      "Training with parameters: {'lr': 0.01, 'batch_size': 64}\n",
      "Epoch [2/70], Train Loss: 6.5547, Train R2: 0.8696, Val Loss: 5.5659, Val R2: 0.8881\n",
      "Epoch [4/70], Train Loss: 4.0581, Train R2: 0.9193, Val Loss: 3.6768, Val R2: 0.9261\n",
      "Epoch [6/70], Train Loss: 3.5493, Train R2: 0.9294, Val Loss: 3.3646, Val R2: 0.9323\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[195], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTuning Linear Regression model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m linear_best_params, linear_best_loss, linear_best_model, linear_best_r2 \u001b[38;5;241m=\u001b[39m \u001b[43mtune_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLinearRegression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_param_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Linear Regression parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlinear_best_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Linear Regression validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlinear_best_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[192], line 23\u001b[0m, in \u001b[0;36mtune_hyperparameters\u001b[0;34m(model_class, param_grid)\u001b[0m\n\u001b[1;32m     20\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mcurrent_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mcurrent_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m val_loss, val_r2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n\u001b[1;32m     26\u001b[0m     best_val_r2 \u001b[38;5;241m=\u001b[39m val_r2\n",
      "Cell \u001b[0;32mIn[190], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m train_true \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     10\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniforge3/envs/spark_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/spark_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/spark_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/spark_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/spark_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/spark_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/spark_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniforge3/envs/spark_env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\nTuning Linear Regression model...\")\n",
    "linear_best_params, linear_best_loss, linear_best_model, linear_best_r2 = tune_hyperparameters(LinearRegression, linear_param_grid)\n",
    "print(f\"Best Linear Regression parameters: {linear_best_params}\")\n",
    "print(f\"Best Linear Regression validation loss: {linear_best_loss}\")\n",
    "print(f\"Best Linear Regression validation R2: {linear_best_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best models\n",
    "torch.save(mlp_best_model, 'best_mlp_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.9863\n",
      "Mean Squared Error: 0.6814\n",
      "Root Mean Squared Error: 0.8255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/pr4fd47s1y50xqg4b8qxdsk80000gn/T/ipykernel_17181/4197647866.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./best_mlp_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "test_function(\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
